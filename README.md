# avrioc_CaseStudy
CaseStudy

# **ðŸš€ Kafka â†’ MongoDB â†’ CSV Data Pipeline in Gitpod**

This project sets up a **real-time data pipeline** using Kafka, MongoDB, and Flask for visualization. The steps below guide you through **starting, managing, and troubleshooting the pipeline in Gitpod**.

---

## **ðŸ“Œ Prerequisites**
Before running the pipeline, ensure:
- **Docker & Docker Compose** are installed in Gitpod.
- **Python Virtual Environment (`venv`)** is set up.
- **Kafka, MongoDB, and Zookeeper** are running correctly.

---

## **ðŸ“Œ Step 1: Set Up Gitpod and Environment**
### **ðŸ”¹ Create a New Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate
```

### **ðŸ”¹ Install Dependencies**
```bash
pip install -r requirements.txt
```

---

## **ðŸ“Œ Step 2: Start Kafka, MongoDB, and Zookeeper**
Run the following command to **start all services**:
```bash
docker-compose up -d
```
âœ… This starts **Kafka, Zookeeper, and MongoDB** in the background.

### **ðŸ”¹ Verify Running Containers**
```bash
docker ps
```
âœ… Expected output should list **Kafka, Zookeeper, and MongoDB**.

---

## **ðŸ“Œ Step 3: Run Kafka Producer (Data Generator)**
```bash
python data_generator.py
```
âœ… Expected Output:
```plaintext
Produced: {'user_id': 12, 'item_id': 45, 'interaction_type': 'click', 'timestamp': '2025-02-28T12:34:56'}
```

---

## **ðŸ“Œ Step 4: Run Kafka Consumer (Store Data in MongoDB)**
```bash
python kafka_consumer.py
```
âœ… Expected Output:
```plaintext
Updated MongoDB with user 12 interactions.
```

---

## **ðŸ“Œ Step 5: Verify Data in MongoDB**
### **ðŸ”¹ Connect to MongoDB**
```bash
docker exec -it $(docker ps -qf "name=mongo") mongosh -u admin -p password --authenticationDatabase admin
```

### **ðŸ”¹ Check Stored Data**
```javascript
use analytics
db.aggregated_interactions.find().pretty()
```
âœ… Expected Output:
```json
{
  "metric": "user_avg",
  "average_interactions": 5.2
}
```

---

## **ðŸ“Œ Step 6: Export Data from MongoDB to CSV**
### **ðŸ”¹ Export Data**
```bash
docker exec -it $(docker ps -qf "name=mongo") mongoexport --db analytics --collection aggregated_interactions --type=csv --fields metric,average_interactions --out /tmp/mongo_data.csv
```

### **ðŸ”¹ Copy CSV File to Gitpod Workspace**
```bash
docker cp $(docker ps -qf "name=mongo"):/tmp/mongo_data.csv ./mongo_data.csv
```

### **ðŸ”¹ Verify CSV Content**
```bash
cat mongo_data.csv
```
âœ… **Now, `mongo_data.csv` should be ready for download!**

---

## **ðŸ“Œ Step 7: Download CSV and Open in Excel**
1. **Locate `mongo_data.csv` in Gitpod File Explorer.**
2. **Right-click â†’ Download.**
3. **Open in Microsoft Excel.** ðŸŽ‰

---

## **ðŸ“Œ Step 8: Start Flask API and View Dashboard**
```bash
python dashboard.py
```
âœ… Open `http://localhost:5001/metrics` in your browser to view real-time data.

---

## **ðŸ“Œ Step 9: Open Multiple Terminals in Gitpod**
To manage different services efficiently:
- **New Terminal:** `Ctrl + Shift + ``
- **Split Terminal:** `Ctrl + Shift + 5`

âœ… Example Setup:
| Terminal | Command |
|----------|---------|
| **Terminal 1** | `docker-compose up -d` |
| **Terminal 2** | `python data_generator.py` |
| **Terminal 3** | `python kafka_consumer.py` |
| **Terminal 4** | `python dashboard.py` |

---

## **ðŸš€ Troubleshooting**
| Issue | Solution |
|-------|----------|
| `ModuleNotFoundError: No module named 'kafka'` | Run `pip install kafka-python` and retry. |
| MongoDB connection errors | Check credentials in `docker-compose.yml`. |
| No data in MongoDB | Ensure `data_generator.py` and `kafka_consumer.py` are running. |
| Empty CSV file | Verify `db.aggregated_interactions.find().pretty()` has data. |

---

## **ðŸŽ¯ Next Steps**
ðŸ”¹ **Want to automate data exports?**
ðŸ”¹ **Need real-time analytics in Google Sheets?**
Let me know how I can assist! ðŸš€ðŸ”¥


# **ðŸš€ Kafka â†’ MongoDB â†’ CSV Data Pipeline in Gitpod**

This project sets up a **real-time data pipeline** using Kafka, MongoDB, and Flask for visualization. The steps below guide you through **starting, managing, and troubleshooting the pipeline in Gitpod**.

---

## **ðŸ“Œ Prerequisites**
Before running the pipeline, ensure:
- **Docker & Docker Compose** are installed in Gitpod.
- **Python Virtual Environment (`venv`)** is set up.
- **Kafka, MongoDB, and Zookeeper** are running correctly.

---

## **ðŸ“Œ Step 1: Set Up Gitpod and Environment**
### **ðŸ”¹ Create a New Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate
```

### **ðŸ”¹ Install Dependencies**
```bash
pip install -r requirements.txt
```

---

## **ðŸ“Œ Step 2: Set Up Kafka, MongoDB, and Zookeeper**
### **ðŸ”¹ Create `docker-compose.yml`**
```yaml
version: "3.8"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
    ports:
      - "9092:9092"

  mongodb:
    image: mongo:latest
    restart: always
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    command: ["mongod", "--auth", "--setParameter", "enableLocalhostAuthBypass=false"]
    volumes:
      - mongodb_data:/data/db

volumes:
  mongodb_data:
```
### **ðŸ”¹ Start Services**
```bash
docker-compose up -d
```
âœ… This starts **Kafka, Zookeeper, and MongoDB** in the background.

### **ðŸ”¹ Verify Running Containers**
```bash
docker ps
```
âœ… Expected output should list **Kafka, Zookeeper, and MongoDB**.

---

## **ðŸ“Œ Step 3: Run Kafka Producer (Data Generator)**
### **ðŸ”¹ Create `data_generator.py`**
```python
import json
import random
import time
from datetime import datetime
from kafka import KafkaProducer

KAFKA_BROKER = "localhost:9092"
KAFKA_TOPIC = "user_interactions"

producer = KafkaProducer(
    bootstrap_servers=KAFKA_BROKER,
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

INTERACTION_TYPES = ["click", "view", "purchase"]

def generate_interaction():
    return {
        "user_id": random.randint(1, 1000),
        "item_id": random.randint(1, 500),
        "interaction_type": random.choice(INTERACTION_TYPES),
        "timestamp": datetime.now().isoformat()
    }

def produce_data(rate_per_sec=10):
    while True:
        interaction = generate_interaction()
        producer.send(KAFKA_TOPIC, value=interaction)
        print(f"Produced: {interaction}")
        time.sleep(1 / rate_per_sec)

if __name__ == "__main__":
    produce_data(rate_per_sec=50)
```
### **ðŸ”¹ Run Producer**
```bash
python data_generator.py
```

---

## **ðŸ“Œ Step 4: Run Kafka Consumer (Store Data in MongoDB)**
### **ðŸ”¹ Create `kafka_consumer.py`**
```python
import json
from kafka import KafkaConsumer
from pymongo import MongoClient

KAFKA_BROKER = "localhost:9092"
KAFKA_TOPIC = "user_interactions"
MONGO_URI = "mongodb://admin:password@localhost:27017/?authSource=admin"
MONGO_DB = "analytics"
MONGO_COLLECTION = "aggregated_interactions"

consumer = KafkaConsumer(
    KAFKA_TOPIC,
    bootstrap_servers=KAFKA_BROKER,
    auto_offset_reset="earliest",
    value_deserializer=lambda v: json.loads(v.decode("utf-8"))
)

mongo_client = MongoClient(MONGO_URI)
db = mongo_client[MONGO_DB]
collection = db[MONGO_COLLECTION]

for message in consumer:
    data = message.value
    collection.insert_one(data)
    print(f"Inserted: {data}")
```
### **ðŸ”¹ Run Consumer**
```bash
python kafka_consumer.py
```

---

## **ðŸ“Œ Step 5: Verify and Export Data**
### **ðŸ”¹ Verify Data in MongoDB**
```bash
docker exec -it $(docker ps -qf "name=mongo") mongosh -u admin -p password --authenticationDatabase admin
```
```javascript
use analytics
db.aggregated_interactions.find().pretty()
```
### **ðŸ”¹ Export Data to CSV**
```bash
docker exec -it $(docker ps -qf "name=mongo") mongoexport --db analytics --collection aggregated_interactions --type=csv --fields metric,average_interactions --out /tmp/mongo_data.csv
docker cp $(docker ps -qf "name=mongo"):/tmp/mongo_data.csv ./mongo_data.csv
```

---

## **ðŸ“Œ Step 6: Start Flask API and View Dashboard**
### **ðŸ”¹ Create `dashboard.py`**
```python
from flask import Flask, jsonify
from pymongo import MongoClient

app = Flask(__name__)

mongo_client = MongoClient("mongodb://admin:password@localhost:27017/?authSource=admin")
db = mongo_client["analytics"]
collection = db["aggregated_interactions"]

@app.route("/metrics", methods=["GET"])
def get_metrics():
    data = list(collection.find({}, {"_id": 0}))
    return jsonify(data)

if __name__ == "__main__":
    app.run(debug=True, port=5001)
```


### **ðŸ”¹ Run Flask API**
```bash
python dashboard.py
```
âœ… Open `http://localhost:5001/metrics` to view real-time data.

---

## **ðŸ“Œ Step 7: Download CSV and Open in Excel**
1. **Locate `mongo_data.csv` in Gitpod File Explorer.**
2. **Right-click â†’ Download.**
3. **Open in Microsoft Excel.** ðŸŽ‰

